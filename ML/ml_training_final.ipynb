{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Model Training: Preferences ‚Üí Cost Parameters\n",
        "\n",
        "Train a regression model to translate user preferences into ODL cost parameters.\n",
        "\n",
        "## Goal:\n",
        "- **Input:** User preferences (parking, time, distance importance)\n",
        "- **Output:** Cost parameters (costPerTravelHour, costPerKm, parking_multiplier)\n",
        "\n",
        "## Data:\n",
        "- **12,000 training samples** from Pareto Top-N selection\n",
        "- Each preference matched to multiple cost/solution pairs\n",
        "- Weighted by distance (closer = more important)\n",
        "\n",
        "## Approach:\n",
        "1. Load training data with sample weights\n",
        "2. Train multiple regression models (with weighted samples)\n",
        "3. Evaluate and compare\n",
        "4. Save best model for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Imports successful\n",
            "‚úì Output directory: ml_training_results_20251106_085226\n",
            "‚úì Figures will be saved to: ml_training_results_20251106_085226/figures\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Create output directory with timestamp\n",
        "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "OUTPUT_DIR = f'ml_training_results_{TIMESTAMP}'\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures')\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Figure counter for auto-naming\n",
        "figure_counter = {'count': 0}\n",
        "\n",
        "def save_figure(name=None, dpi=300):\n",
        "    \"\"\"Automatically save the current matplotlib figure\"\"\"\n",
        "    if name is None:\n",
        "        figure_counter['count'] += 1\n",
        "        name = f'figure_{figure_counter[\"count\"]:02d}'\n",
        "    \n",
        "    filepath = os.path.join(FIGURES_DIR, f'{name}.png')\n",
        "    plt.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
        "    print(f\"  üíæ Saved: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "print(\"‚úì Imports successful\")\n",
        "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"‚úì Figures will be saved to: {FIGURES_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data file\n",
        "TRAINING_DATA_FILE = 'training_data.json'\n",
        "\n",
        "# Model save path\n",
        "MODEL_SAVE_PATH = 'preference_to_cost_model.pkl'\n",
        "SCALER_SAVE_PATH = 'preference_scaler.pkl'\n",
        "\n",
        "# Train/test split\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Training data: {TRAINING_DATA_FILE}\")\n",
        "print(f\"  Test size: {TEST_SIZE*100:.0f}%\")\n",
        "print(f\"  Model output: {MODEL_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "with open(TRAINING_DATA_FILE, 'r') as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(training_data)} training samples\")\n",
        "\n",
        "# Show first sample\n",
        "print(\"\\nSample structure:\")\n",
        "print(json.dumps(training_data[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame for easier analysis\n",
        "data = []\n",
        "for sample in training_data:\n",
        "    row = {\n",
        "        # Inputs\n",
        "        'parking_importance': sample['preferences']['parking_importance'],\n",
        "        'time_importance': sample['preferences']['time_importance'],\n",
        "        'distance_importance': sample['preferences']['distance_importance'],\n",
        "        # Outputs\n",
        "        'costPerTravelHour': sample['costs']['costPerTravelHour'],\n",
        "        'costPerKm': sample['costs']['costPerKm'],\n",
        "        'parking_multiplier': sample['costs']['parking_multiplier'],\n",
        "        # Metadata\n",
        "        'pool_id': sample['metadata']['pool_id'],\n",
        "        'pareto_distance': sample['metadata']['pareto_distance'],\n",
        "        'rank_for_preference': sample['metadata']['rank_for_preference'],\n",
        "        # Sample weight\n",
        "        'sample_weight': sample['sample_weight']\n",
        "    }\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Statistics:\\n\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check output diversity\n",
        "unique_costs = df[['costPerTravelHour', 'costPerKm', 'parking_multiplier']].drop_duplicates()\n",
        "print(f\"\\nUnique cost combinations: {len(unique_costs)}\")\n",
        "print(f\"Average samples per unique output: {len(df) / len(unique_costs):.1f}\")\n",
        "\n",
        "# Check preference diversity\n",
        "unique_prefs = df[['parking_importance', 'time_importance', 'distance_importance']].drop_duplicates()\n",
        "print(f\"Unique preference combinations: {len(unique_prefs)}\")\n",
        "print(f\"Average samples per unique preference: {len(df) / len(unique_prefs):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sample weights\n",
        "print(\"Sample Weight Statistics:\")\n",
        "print(f\"  Mean: {df['sample_weight'].mean():.3f}\")\n",
        "print(f\"  Median: {df['sample_weight'].median():.3f}\")\n",
        "print(f\"  Min: {df['sample_weight'].min():.3f}\")\n",
        "print(f\"  Max: {df['sample_weight'].max():.3f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['sample_weight'], bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.title('Sample Weight Distribution')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df['pareto_distance'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "plt.title('Pareto Distance Distribution')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure('sample_weights_and_distances')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Sample weights give higher importance to closer matches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize input distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].hist(df['parking_importance'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Parking Importance Distribution')\n",
        "axes[0].set_xlabel('Importance')\n",
        "\n",
        "axes[1].hist(df['time_importance'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[1].set_title('Time Importance Distribution')\n",
        "axes[1].set_xlabel('Importance')\n",
        "\n",
        "axes[2].hist(df['distance_importance'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[2].set_title('Distance Importance Distribution')\n",
        "axes[2].set_xlabel('Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure('input_distributions')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Inputs are well-distributed (Dirichlet sampling)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize output distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].hist(df['costPerTravelHour'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[0].set_title(f'Cost Per Travel Hour\\n({len(df[\"costPerTravelHour\"].unique())} unique values)')\n",
        "axes[0].set_xlabel('Cost ($/hour)')\n",
        "\n",
        "axes[1].hist(df['costPerKm'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1].set_title(f'Cost Per Km\\n({len(df[\"costPerKm\"].unique())} unique values)')\n",
        "axes[1].set_xlabel('Cost ($/km)')\n",
        "\n",
        "axes[2].hist(df['parking_multiplier'], bins=30, edgecolor='black', alpha=0.7, color='red')\n",
        "axes[2].set_title(f'Parking Multiplier\\n({len(df[\"parking_multiplier\"].unique())} unique values)')\n",
        "axes[2].set_xlabel('Multiplier')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure('output_distributions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze rank distribution (how many top-1, top-2, top-3 etc matches)\n",
        "rank_counts = df['rank_for_preference'].value_counts().sort_index()\n",
        "print(\"Distribution of Ranks (Top-N matches per preference):\")\n",
        "for rank, count in rank_counts.items():\n",
        "    print(f\"  Rank {rank}: {count} samples ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(rank_counts.index, rank_counts.values, edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribution of Sample Ranks')\n",
        "plt.xlabel('Rank (1 = closest match)')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "save_figure('rank_distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features (X) and targets (y)\n",
        "X = df[['parking_importance', 'time_importance', 'distance_importance']].values\n",
        "y = df[['costPerTravelHour', 'costPerKm', 'parking_multiplier']].values\n",
        "\n",
        "# Extract sample weights\n",
        "sample_weights = df['sample_weight'].values\n",
        "\n",
        "print(f\"Input shape (X): {X.shape}\")\n",
        "print(f\"Output shape (y): {y.shape}\")\n",
        "print(f\"Sample weights shape: {sample_weights.shape}\")\n",
        "print(f\"\\n‚úì Ready for training with weighted samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
        "    X, y, sample_weights, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"\\nTest set is {TEST_SIZE*100:.0f}% of data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Neural Network: scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úì Features scaled for Neural Network\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models to train\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    'Neural Network': MLPRegressor(\n",
        "        hidden_layer_sizes=(64, 32, 16),\n",
        "        activation='relu',\n",
        "        max_iter=500,\n",
        "        random_state=RANDOM_STATE,\n",
        "        early_stopping=True\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Models to train:\")\n",
        "for name in models.keys():\n",
        "    print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models with sample weights\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Wrap in MultiOutputRegressor\n",
        "    multi_model = MultiOutputRegressor(model)\n",
        "    \n",
        "    # Use scaled data for Neural Network, regular data for others\n",
        "    if name == 'Neural Network':\n",
        "        multi_model.fit(X_train_scaled, y_train, sample_weight=weights_train)\n",
        "        y_pred = multi_model.predict(X_test_scaled)\n",
        "    else:\n",
        "        multi_model.fit(X_train, y_train, sample_weight=weights_train)\n",
        "        y_pred = multi_model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics for each output\n",
        "    output_names = ['costPerTravelHour', 'costPerKm', 'parking_multiplier']\n",
        "    metrics = {}\n",
        "    \n",
        "    for i, output_name in enumerate(output_names):\n",
        "        mse = mean_squared_error(y_test[:, i], y_pred[:, i])\n",
        "        mae = mean_absolute_error(y_test[:, i], y_pred[:, i])\n",
        "        r2 = r2_score(y_test[:, i], y_pred[:, i])\n",
        "        \n",
        "        metrics[output_name] = {\n",
        "            'MSE': mse,\n",
        "            'RMSE': np.sqrt(mse),\n",
        "            'MAE': mae,\n",
        "            'R¬≤': r2\n",
        "        }\n",
        "    \n",
        "    # Overall R¬≤\n",
        "    overall_r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': multi_model,\n",
        "        'predictions': y_pred,\n",
        "        'metrics': metrics,\n",
        "        'overall_r2': overall_r2\n",
        "    }\n",
        "    \n",
        "    print(f\"  Overall R¬≤ score: {overall_r2:.4f}\")\n",
        "\n",
        "print(\"\\n‚úì All models trained with sample weights!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, result in results.items():\n",
        "    for output_name, metrics in result['metrics'].items():\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Output': output_name,\n",
        "            'R¬≤': metrics['R¬≤'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAE': metrics['MAE']\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nDetailed Model Comparison:\\n\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Overall comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Overall R¬≤ Scores:\")\n",
        "for model_name, result in results.items():\n",
        "    print(f\"  {model_name:20s}: {result['overall_r2']:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize R¬≤ scores\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: R¬≤ by model and output\n",
        "pivot_df = comparison_df.pivot(index='Output', columns='Model', values='R¬≤')\n",
        "pivot_df.plot(kind='bar', ax=axes[0], edgecolor='black', alpha=0.8)\n",
        "axes[0].set_title('R¬≤ Score by Model and Output', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('R¬≤ Score')\n",
        "axes[0].set_xlabel('Output Parameter')\n",
        "axes[0].legend(title='Model')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Plot 2: Overall R¬≤\n",
        "overall_r2_data = [result['overall_r2'] for result in results.values()]\n",
        "axes[1].bar(results.keys(), overall_r2_data, edgecolor='black', alpha=0.8)\n",
        "axes[1].set_title('Overall R¬≤ Score by Model', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('R¬≤ Score')\n",
        "axes[1].set_xlabel('Model')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure('model_comparison_r2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model based on overall R¬≤\n",
        "best_model_name = max(results.items(), key=lambda x: x[1]['overall_r2'])[0]\n",
        "best_model = results[best_model_name]['model']\n",
        "best_r2 = results[best_model_name]['overall_r2']\n",
        "\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
        "print(f\"   Overall R¬≤ Score: {best_r2:.4f}\")\n",
        "print(\"\\nPer-output performance:\")\n",
        "for output_name, metrics in results[best_model_name]['metrics'].items():\n",
        "    print(f\"  {output_name}:\")\n",
        "    print(f\"    R¬≤ = {metrics['R¬≤']:.4f}\")\n",
        "    print(f\"    MAE = {metrics['MAE']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prediction Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actual vs Predicted plots\n",
        "y_pred_best = results[best_model_name]['predictions']\n",
        "output_names = ['costPerTravelHour', 'costPerKm', 'parking_multiplier']\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for i, (ax, name) in enumerate(zip(axes, output_names)):\n",
        "    ax.scatter(y_test[:, i], y_pred_best[:, i], alpha=0.3, s=10)\n",
        "    \n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_test[:, i].min(), y_pred_best[:, i].min())\n",
        "    max_val = max(y_test[:, i].max(), y_pred_best[:, i].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
        "    \n",
        "    ax.set_xlabel('Actual')\n",
        "    ax.set_ylabel('Predicted')\n",
        "    ax.set_title(f'{name}\\n(R¬≤ = {results[best_model_name][\"metrics\"][name][\"R¬≤\"]:.4f})')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'{best_model_name} - Actual vs Predicted', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "save_figure('actual_vs_predicted')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Points close to red line = good predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Residual plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for i, (ax, name) in enumerate(zip(axes, output_names)):\n",
        "    residuals = y_test[:, i] - y_pred_best[:, i]\n",
        "    ax.scatter(y_pred_best[:, i], residuals, alpha=0.3, s=10)\n",
        "    ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "    ax.set_xlabel('Predicted Value')\n",
        "    ax.set_ylabel('Residuals')\n",
        "    ax.set_title(f'{name} Residuals')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'{best_model_name} - Residual Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "save_figure('residual_analysis')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì If residuals are randomly scattered around 0, the model is unbiased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance (for tree-based models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance for Random Forest or Gradient Boosting\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "    feature_names = ['parking_importance', 'time_importance', 'distance_importance']\n",
        "    \n",
        "    # Average feature importances across all outputs\n",
        "    importances = np.mean([estimator.feature_importances_ \n",
        "                          for estimator in best_model.estimators_], axis=0)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(feature_names, importances, edgecolor='black', alpha=0.7)\n",
        "    plt.title(f'Feature Importance - {best_model_name}')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    save_figure('feature_importance')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Feature Importances:\")\n",
        "    for name, imp in zip(feature_names, importances):\n",
        "        print(f\"  {name}: {imp:.4f}\")\n",
        "else:\n",
        "    print(f\"Feature importance not available for {best_model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test on New Preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test predictions on some example preferences\n",
        "test_cases = [\n",
        "    {'name': 'High parking priority', \n",
        "     'parking_importance': 0.8, 'time_importance': 0.1, 'distance_importance': 0.1},\n",
        "    {'name': 'High time priority', \n",
        "     'parking_importance': 0.1, 'time_importance': 0.8, 'distance_importance': 0.1},\n",
        "    {'name': 'High distance priority', \n",
        "     'parking_importance': 0.1, 'time_importance': 0.1, 'distance_importance': 0.8},\n",
        "    {'name': 'Balanced priorities', \n",
        "     'parking_importance': 0.33, 'time_importance': 0.33, 'distance_importance': 0.34}\n",
        "]\n",
        "\n",
        "print(\"Test Predictions:\\n\")\n",
        "\n",
        "for case in test_cases:\n",
        "    X_new = np.array([[case['parking_importance'], \n",
        "                      case['time_importance'], \n",
        "                      case['distance_importance']]])\n",
        "    \n",
        "    # Scale if using Neural Network\n",
        "    if best_model_name == 'Neural Network':\n",
        "        X_new = scaler.transform(X_new)\n",
        "    \n",
        "    prediction = best_model.predict(X_new)[0]\n",
        "    \n",
        "    print(f\"{case['name']}:\")\n",
        "    print(f\"  Input: parking={case['parking_importance']:.2f}, \"\n",
        "          f\"time={case['time_importance']:.2f}, \"\n",
        "          f\"distance={case['distance_importance']:.2f}\")\n",
        "    print(f\"  Predicted costs:\")\n",
        "    print(f\"    costPerTravelHour = {prediction[0]:.2f}\")\n",
        "    print(f\"    costPerKm = {prediction[1]:.4f}\")\n",
        "    print(f\"    parking_multiplier = {prediction[2]:.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "model_path = os.path.join(OUTPUT_DIR, MODEL_SAVE_PATH)\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "print(f\"‚úì Best model ({best_model_name}) saved to: {model_path}\")\n",
        "\n",
        "# Save scaler if using Neural Network\n",
        "if best_model_name == 'Neural Network':\n",
        "    scaler_path = os.path.join(OUTPUT_DIR, SCALER_SAVE_PATH)\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    print(f\"‚úì Scaler saved to: {scaler_path}\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'model_type': best_model_name,\n",
        "    'overall_r2': float(best_r2),\n",
        "    'training_samples': len(X_train),\n",
        "    'test_samples': len(X_test),\n",
        "    'unique_outputs': int(len(unique_costs)),\n",
        "    'unique_preferences': int(len(unique_prefs)),\n",
        "    'used_sample_weights': True,\n",
        "    'metrics': {name: {k: float(v) for k, v in metrics.items()} \n",
        "                for name, metrics in results[best_model_name]['metrics'].items()},\n",
        "    'input_features': ['parking_importance', 'time_importance', 'distance_importance'],\n",
        "    'output_features': ['costPerTravelHour', 'costPerKm', 'parking_multiplier'],\n",
        "    'requires_scaling': best_model_name == 'Neural Network',\n",
        "    'timestamp': TIMESTAMP\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(OUTPUT_DIR, 'model_metadata.json')\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Metadata saved to: {metadata_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Usage Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: How to load and use the saved model\n",
        "print(\"Example usage code:\\n\")\n",
        "print(\"\"\"\n",
        "# Load model\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "with open('preference_to_cost_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Optional: Load scaler if using Neural Network\n",
        "# with open('preference_scaler.pkl', 'rb') as f:\n",
        "#     scaler = pickle.load(f)\n",
        "\n",
        "# Get user preferences\n",
        "preferences = {\n",
        "    'parking_importance': 0.7,\n",
        "    'time_importance': 0.2,\n",
        "    'distance_importance': 0.1\n",
        "}\n",
        "\n",
        "# Prepare input\n",
        "X = np.array([[\n",
        "    preferences['parking_importance'],\n",
        "    preferences['time_importance'],\n",
        "    preferences['distance_importance']\n",
        "]])\n",
        "\n",
        "# Scale if needed\n",
        "# X = scaler.transform(X)  # Only for Neural Network\n",
        "\n",
        "# Predict costs\n",
        "costs = model.predict(X)[0]\n",
        "\n",
        "result = {\n",
        "    'costPerTravelHour': costs[0],\n",
        "    'costPerKm': costs[1],\n",
        "    'parking_multiplier': costs[2]\n",
        "}\n",
        "\n",
        "print(result)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Package Everything üì¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Create a summary report\n",
        "summary_report = f\"\"\"# ML Model Training Results\n",
        "\n",
        "**Training Date:** {TIMESTAMP}\n",
        "**Output Directory:** {OUTPUT_DIR}\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Summary\n",
        "\n",
        "- **Total Training Samples:** {len(training_data):,}\n",
        "- **Training Set:** {len(X_train):,} samples\n",
        "- **Test Set:** {len(X_test):,} samples\n",
        "- **Unique Cost Combinations:** {len(unique_costs):,}\n",
        "- **Unique Preference Combinations:** {len(unique_prefs):,}\n",
        "- **Used Sample Weights:** Yes (distance-based weighting)\n",
        "\n",
        "---\n",
        "\n",
        "## Model Performance\n",
        "\n",
        "### Best Model: {best_model_name}\n",
        "\n",
        "**Overall R¬≤ Score:** {best_r2:.4f}\n",
        "\n",
        "### Per-Output Performance:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for output_name, metrics in results[best_model_name]['metrics'].items():\n",
        "    summary_report += f\"\"\"\n",
        "#### {output_name}\n",
        "- R¬≤ Score: {metrics['R¬≤']:.4f}\n",
        "- RMSE: {metrics['RMSE']:.4f}\n",
        "- MAE: {metrics['MAE']:.4f}\n",
        "\"\"\"\n",
        "\n",
        "summary_report += f\"\"\"\n",
        "---\n",
        "\n",
        "## Model Comparison\n",
        "\n",
        "| Model | Overall R¬≤ |\n",
        "|-------|------------|\n",
        "\"\"\"\n",
        "\n",
        "for model_name, result in sorted(results.items(), key=lambda x: x[1]['overall_r2'], reverse=True):\n",
        "    summary_report += f\"| {model_name} | {result['overall_r2']:.4f} |\\n\"\n",
        "\n",
        "summary_report += f\"\"\"\n",
        "---\n",
        "\n",
        "## Files Generated\n",
        "\n",
        "### Model Files\n",
        "- `{MODEL_SAVE_PATH}` - Trained model (pickle format)\n",
        "\"\"\"\n",
        "\n",
        "if best_model_name == 'Neural Network':\n",
        "    summary_report += f\"- `{SCALER_SAVE_PATH}` - Feature scaler (required for predictions)\\n\"\n",
        "\n",
        "summary_report += f\"\"\"- `model_metadata.json` - Model metadata and metrics\n",
        "\n",
        "### Figures ({figure_counter['count']} total)\n",
        "\"\"\"\n",
        "\n",
        "# List all saved figures\n",
        "figure_files = sorted([f for f in os.listdir(FIGURES_DIR) if f.endswith('.png')])\n",
        "for fig_file in figure_files:\n",
        "    summary_report += f\"- `figures/{fig_file}`\\n\"\n",
        "\n",
        "summary_report += \"\"\"\n",
        "---\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "```python\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "with open('preference_to_cost_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\"\"\"\n",
        "\n",
        "if best_model_name == 'Neural Network':\n",
        "    summary_report += \"\"\"\n",
        "# Load the scaler (Neural Network only)\n",
        "with open('preference_scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\"\"\"\n",
        "\n",
        "summary_report += \"\"\"\n",
        "# Define user preferences\n",
        "preferences = np.array([[\n",
        "    0.7,  # parking_importance\n",
        "    0.2,  # time_importance\n",
        "    0.1   # distance_importance\n",
        "]])\n",
        "\"\"\"\n",
        "\n",
        "if best_model_name == 'Neural Network':\n",
        "    summary_report += \"\"\"\n",
        "# Scale inputs (Neural Network only)\n",
        "preferences = scaler.transform(preferences)\n",
        "\"\"\"\n",
        "\n",
        "summary_report += \"\"\"\n",
        "# Predict costs\n",
        "costs = model.predict(preferences)[0]\n",
        "\n",
        "print(f\"costPerTravelHour: {costs[0]:.2f}\")\n",
        "print(f\"costPerKm: {costs[1]:.4f}\")\n",
        "print(f\"parking_multiplier: {costs[2]:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "- Model was trained with sample weights based on Pareto distance\n",
        "- Closer preference-cost matches received higher importance during training\n",
        "- All figures show model performance on the test set\n",
        "- Ready for production deployment\n",
        "\"\"\"\n",
        "\n",
        "# Save summary report\n",
        "summary_path = os.path.join(OUTPUT_DIR, 'TRAINING_SUMMARY.md')\n",
        "with open(summary_path, 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(f\"‚úÖ Training summary saved to: {summary_path}\")\n",
        "\n",
        "# Create a zip file with everything\n",
        "zip_filename = f'{OUTPUT_DIR}.zip'\n",
        "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nüì¶ All results packaged!\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"OUTPUT DIRECTORY: {OUTPUT_DIR}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nContents:\")\n",
        "print(f\"  üìÅ {MODEL_SAVE_PATH}\")\n",
        "if best_model_name == 'Neural Network':\n",
        "    print(f\"  üìÅ {SCALER_SAVE_PATH}\")\n",
        "print(f\"  üìÅ model_metadata.json\")\n",
        "print(f\"  üìÅ TRAINING_SUMMARY.md\")\n",
        "print(f\"  üìÅ figures/ ({figure_counter['count']} images)\")\n",
        "print(f\"\\n  üì¶ {zip_filename} (all files packaged)\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"\\n‚úÖ Training complete! All results saved and packaged.\")\n",
        "print(f\"\\nüí° Check {summary_path} for detailed results!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "‚úÖ Trained on 12,000 weighted samples from Pareto Top-N selection\n",
        "‚úÖ Sample weights prioritize closer preference-cost matches\n",
        "‚úÖ Evaluated Random Forest, Gradient Boosting, and Neural Network\n",
        "‚úÖ Saved best model for deployment\n",
        "‚úÖ Model translates preferences ‚Üí cost parameters\n",
        "‚úÖ **All figures and model automatically saved to timestamped directory**\n",
        "‚úÖ **Complete package available as zip file**\n",
        "\n",
        "### Key Findings:\n",
        "- Training used weighted samples (closer matches = higher importance)\n",
        "- Model learned from diverse preference-cost mappings\n",
        "- Ready for production deployment\n",
        "- All analysis figures saved for documentation\n",
        "\n",
        "### Next Steps:\n",
        "1. Integrate model into your application\n",
        "2. Get user preferences (parking, time, distance importance)\n",
        "3. Predict cost parameters using model\n",
        "4. Send costs to ODL API for route optimization\n",
        "5. Return optimized routes to user!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
